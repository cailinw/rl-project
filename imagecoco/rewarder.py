import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


# https://github.com/nurpeiis/LeakGAN-PyTorch/blob/master/Discriminator.py

class CostModel(nn.Module):
	def __init__(self, seq_length, vocab_size,
			input_size, hidden_size1, hidden_size2,
			dropout_prob, l2_reg_lambda, emb_dim=1):
		super(CostModel, self).__init__()
		self.seq_length = seq_length
		self.vocab_size = vocab_size
		self.input_size = input_size
		self.hidden_size = hidden_size
		# self.hidden_size1 = hidden_size1
		# self.hidden_size2 = hidden_size2
		self.dropout_prob = dropout_prob
		self.l2_reg_lambda = l2_reg_lambda
		# self.emb_dim = emb_dim

		self.fc_i = nn.Linear(input_size, hidden_size)
		self.fc_h = nn.Linear(hidden_size, hidden_size)
		self.fc_o = nn.Linear(hidden_size2, vocab_size)  # Outputs probs/logits for each word in vocab

	def forward(self, x):
		'''
		x : (batch_size, input_size)
		'''
		output = F.relu(self.fc_i(x))

		output = F.relu(self.fc_h(x))
		output = F.relu(self.fc_h(x))
		output = F.relu(self.fc_h(x))
		output = F.relu(self.fc_h(x))

		output = self.fc_o(output)
		return output  # (batch_size, vocab_size)

class Rewarder():
	def __init__(self,
			seq_length,
			hidden_size1,
			hidden_size2,
			vocab_size,
			batch_size,
			learning_rate
		):

		self.model = CostModel(seq_length, hidden_size1, hidden_size2, vocab_size)
		self.batch_size = batch_size
		self.learning_rate = learning_rate
		self.optimizer = torch.optim.Adam(model.parameters(), self.learning_rate)


	def train_step(self, x_text, generator):
		self.optimizer.zero_grad()

		real_x = x_text[self.batch_size//2:] # demonstrations
		fake_x = x_text[:self.batch_size//2] # generated by generator

		# TODO: Compute weight
		real_weight = np.array([1.0 / (self.batch_size // 2)] * (self.batch_size // 2))  # N
		fake_weight = weight[self.batch_size // 2 : ] # M
		weight = np.concatenate((real_weight, fake_weight), axis=0)  # (batch_size, 1)

		# TODO: Compute reward

		# real_probs = F.softmax(self.model(real_x)).detach()
		# fake_probs = F.softmax(self.model(fake_x)).detach()




		loss = np.sum(reward * weight) # TODO: negate this?
		# ?? -> + l2_reg_lambda * (tf.add_n([tf.nn.l2_loss(var) for var in self.r_params if var not in [self.r_embeddings]]))

		loss.backward()
		self.optimizer.step()


